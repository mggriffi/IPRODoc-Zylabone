<?xml version="1.0" encoding="utf-8"?>
<html lang="en-US" xmlns:MadCap="http://www.madcapsoftware.com/Schemas/MadCap.xsd" MadCap:onlyLocalStylesheets="True">
    <head>
        <link href="../../Styles/ADDReview.css" rel="stylesheet" type="text/css" />
    </head>
    <body>
        <h1>Precision and Recall</h1>
        <p>Precision and recall are used to evaluate how well the <a href="329.html">classifier</a> performs. If the classifier reaches a high precision and a high recall, you&#160;will have found (almost) all responsive documents. The found documents are of a high quality and a high quantity. In other words, the 
 
 classifier
 
 is returning accurate results, as well as returning a majority of all responsive results. Sometimes it is more important to correctly classify your documents than getting all of them (high precision, lower recall). Sometimes it is more important to get all responsive documents than getting all of them correct (lower precision, high recall).&#160;
    </p>
        <p>In a project you will have a selection of documents that might be responsive (the   <b>
      presumed positives</b>)&#160;for a specific <a href="319.html">issue</a>. They&#160;are selected by the classifier (the blue circle on the right in the graph below). After review, a selection of those documents is identified as truly responsive (the 
    
  <b>      true positives</b>). The other selected documents are, counter to expectations, not responsive (the
    
    <b>
      false positives</b>). The review results of each <a href="325.html">new training batch</a> enable the classifier to select the presumed positives more accurately.&#160;</p>
        <p>     The documents in the project that are not selected by the classifier are considered not responsive (the      <b style="text-decoration-style: initial;text-decoration-color: initial;">
     presumed negatives</b>). A&#160;selection of those documents will be responsive (the      <b>
        false negatives</b>). The other documents are, as expected, not responsive (the    <b>
          true negatives</b>). 
        </p>
        <p>     It is important to understand, that only the 
        
          documents in the green circle on the left are the truly responsive documents (the actual 
          <b>
            positives</b>) in the project.
        </p>
        <p>    All documents in the project (positives and negatives) are inside the rectangle (90.000 + 15.000 + 60.000 + 10.000 = 175.000).
    </p>
        <p><span class="bodytext"><img src="../../Storage/zylab-one-manual-publication/323-2018-01-18.png" style="width: 500px; height: 277px;" alt="" class="Thumbnail" /><br /></span>
        </p>
        <p><b>   Precision
    </b>
    = quality = 
    
    <i>
      #responsive found / #found&#160;&#160;
    </i>
    = 
    
    <i>
      true positives / (true positives + false positives)
      </i> </p>
        <p> For the review results in the graph displayed above, the calculation will result in a high precision of 86%:
    </p>
        <p> 60.000 / (60.000 + 10.000) = 0,86
    </p>
        <p> The classifier correctly classified 86% of the selected documents (in the blue/right circle) as responsive.
    </p>
        <p><b>   Recall
    </b>
    = quantity =&#160;
    
    <i>
      #responsive found / #responsive&#160;</i>= 
    <i>
      true positives / (true positives + false negatives)
    </i><![CDATA[    
    ]]></p>
        <p>For the review results in the graph displayed above, this calculation will result in a high recall of 80%:
    
    </p>
        <p>  
    60.000 / (60.000 + 15.000) = 0,8
    </p>
        <p> The classifier found 80% of the responsive documents present in the project.
    </p>
        <p>  The problem with the recall calculation is, that you do not know the exact number of truly responsive documents (the documents in the green/left circle) until all documents in the project have been&#160;reviewed. The solution is to use a 
    
      <a href="446.html">Validation Set</a>
    
    to hold your training results against. This will result in an <a href="330.html">Estimated Current Recall</a> per issue. If you have created no Validation Set for your project, you cannot calculate the estimated current recall of an issue.&#160;</p>
    </body>
</html>